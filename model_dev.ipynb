{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Default directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\negas\\OneDrive\\Desktop\\dDevice\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\negas\\OneDrive\\Desktop\\dDevice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>failure</th>\n",
       "      <th>metric1</th>\n",
       "      <th>metric2</th>\n",
       "      <th>metric3</th>\n",
       "      <th>metric4</th>\n",
       "      <th>metric5</th>\n",
       "      <th>metric6</th>\n",
       "      <th>metric8</th>\n",
       "      <th>metric9</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.323358</td>\n",
       "      <td>-0.047478</td>\n",
       "      <td>-0.053516</td>\n",
       "      <td>2.193905</td>\n",
       "      <td>-0.515755</td>\n",
       "      <td>1.485268</td>\n",
       "      <td>-0.039335</td>\n",
       "      <td>-0.028479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.713580</td>\n",
       "      <td>-0.047478</td>\n",
       "      <td>-0.053516</td>\n",
       "      <td>2.193905</td>\n",
       "      <td>-0.515755</td>\n",
       "      <td>1.485268</td>\n",
       "      <td>-0.039335</td>\n",
       "      <td>-0.028479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023124</td>\n",
       "      <td>-0.047478</td>\n",
       "      <td>-0.053516</td>\n",
       "      <td>2.193905</td>\n",
       "      <td>-0.515755</td>\n",
       "      <td>1.485268</td>\n",
       "      <td>-0.039335</td>\n",
       "      <td>-0.028479</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080687</td>\n",
       "      <td>-0.047478</td>\n",
       "      <td>-0.053516</td>\n",
       "      <td>2.193905</td>\n",
       "      <td>-0.515755</td>\n",
       "      <td>1.485278</td>\n",
       "      <td>-0.039335</td>\n",
       "      <td>-0.028479</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.354740</td>\n",
       "      <td>-0.047478</td>\n",
       "      <td>-0.053516</td>\n",
       "      <td>2.193905</td>\n",
       "      <td>-0.515755</td>\n",
       "      <td>1.492086</td>\n",
       "      <td>-0.039335</td>\n",
       "      <td>-0.028479</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  failure   metric1   metric2   metric3   metric4   metric5  \\\n",
       "0           0        0  1.323358 -0.047478 -0.053516  2.193905 -0.515755   \n",
       "1           1        0 -1.713580 -0.047478 -0.053516  2.193905 -0.515755   \n",
       "2           2        0  0.023124 -0.047478 -0.053516  2.193905 -0.515755   \n",
       "3           3        0  0.080687 -0.047478 -0.053516  2.193905 -0.515755   \n",
       "4           4        0 -0.354740 -0.047478 -0.053516  2.193905 -0.515755   \n",
       "\n",
       "    metric6   metric8   metric9  diff  \n",
       "0  1.485268 -0.039335 -0.028479     0  \n",
       "1  1.485268 -0.039335 -0.028479     1  \n",
       "2  1.485268 -0.039335 -0.028479     2  \n",
       "3  1.485278 -0.039335 -0.028479     3  \n",
       "4  1.492086 -0.039335 -0.028479     4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data= pd.read_csv('dev_model.csv')\n",
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "device        0\n",
       "metric1       0\n",
       "metric2       0\n",
       "metric3       0\n",
       "metric4       0\n",
       "metric5       0\n",
       "metric6       0\n",
       "metric8       0\n",
       "metric9       0\n",
       "failure       0\n",
       "diff          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    124388\n",
       "1       106\n",
       "Name: failure, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data['failure'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Target and Feature variable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= dev_data.iloc[:,2:]\n",
    "y= dev_data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SMOTE to address the inbalance of the of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = over.fit_resample(X,y)\n",
    "X, y = under.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 24876, 1: 12438})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Put the three models in dictionary as keys and their respective parameters as values'''\n",
    "param_grid = {\n",
    "    'logistic' : {'C': [1, 10, 100, 1000]},\n",
    "    'rforest' : { 'max_features': [ 'sqrt', 'log2']},\n",
    "    'gboost' : { \"max_features\":[\"log2\",\"sqrt\"],}\n",
    "    \n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Instantiate the models'''\n",
    "logisticReg = LogisticRegression()\n",
    "rforestM = RandomForestClassifier()\n",
    "gboostM = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Put the models again in dictionary with their instance models as values'''\n",
    "models = { \"logistic\": logisticReg, \n",
    "          \"rforest\": rforestM,\n",
    "         \"gboost\" : gboostM\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic\n",
      "rforest\n",
      "gboost\n"
     ]
    }
   ],
   "source": [
    "'''Iterate through the models and their parameters and get the best parameter for each model'''\n",
    "best_models = {}\n",
    "for m in models.keys():\n",
    "    print(m)\n",
    "    grid = GridSearchCV(models[m], param_grid[m], cv=10, scoring='f1')\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_models[m] = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic\n",
      "f1_train=0.77, f1_test=0.76\n",
      "precision_train=0.87, precision=0.86\n",
      "recall_train=0.75, recall=0.74\n",
      "+++++++\n",
      "rforest\n",
      "f1_train=1.0, f1_test=1.0\n",
      "precision_train=1.0, precision=1.0\n",
      "recall_train=1.0, recall=1.0\n",
      "+++++++\n",
      "gboost\n",
      "f1_train=0.96, f1_test=0.96\n",
      "precision_train=0.97, precision=0.96\n",
      "recall_train=0.96, recall=0.96\n",
      "+++++++\n"
     ]
    }
   ],
   "source": [
    "''' Compare best winner model'''\n",
    "for m in best_models.keys():\n",
    "    m_ = best_models[m]\n",
    "    #train\n",
    "    y_pred_train = m_.predict(X_train)\n",
    "    f1_train = round(f1_score(y_train, y_pred_train, average='macro'),2)\n",
    "    precision_train = round(precision_score(y_train, y_pred_train, average='macro'),2)\n",
    "    recall_train = round(recall_score(y_train, y_pred_train, average='macro'),2)\n",
    "    \n",
    "    #Test\n",
    "    y_pred = m_.predict(X_test)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='macro'),2)\n",
    "    precision = round(precision_score(y_test, y_pred, average='macro'),2)\n",
    "    recall = round(recall_score(y_test, y_pred, average='macro'),2)\n",
    "    print(m)\n",
    "    print(f'f1_train={f1_train}, f1_test={f1}')\n",
    "    print(f'precision_train={precision_train}, precision={precision}')\n",
    "    print(f'recall_train={recall_train}, recall={recall}')\n",
    "    \n",
    "    print(\"+++++++\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
